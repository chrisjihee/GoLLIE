/home/chrisjihee/miniforge3/envs/GoLLIE/bin/python -m src.run configs/model_configs/GoLLIE-7B_CodeLLaMA.yaml
INFO:root:Sys args ['/home/chrisjihee/proj/GoLLIE/src/run.py', 'configs/model_configs/GoLLIE-7B_CodeLLaMA.yaml']
INFO:root:Loading yaml config configs/model_configs/GoLLIE-7B_CodeLLaMA.yaml
INFO:root:Loading codellama/CodeLlama-7b-hf model...
INFO:root:Loading model model from codellama/CodeLlama-7b-hf
INFO:root:We will load the model using the following device map: None and max_memory: None
/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
WARNING:root:Tokenizer does not have a pad token, we will use the unk token as pad token.
INFO:root:Bits and Bytes config: {
    "quant_method": "bitsandbytes",
    "_load_in_8bit": false,
    "_load_in_4bit": true,
    "llm_int8_threshold": 6.0,
    "llm_int8_skip_modules": null,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "load_in_4bit": true,
    "load_in_8bit": false
}
WARNING:root:Model codellama/CodeLlama-7b-hf is an decoder-only model. We will load it as a CausalLM model.
Traceback (most recent call last):
  File "/home/chrisjihee/proj/GoLLIE/src/model/patch_models/modeling_flash_llama.py", line 51, in <module>
    from flash_attn.bert_padding import pad_input, unpad_input
  File "/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 10, in <module>
    import flash_attn_2_cuda as flash_attn_cuda
ImportError: /home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZNK3c105Error4whatEv

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/chrisjihee/proj/GoLLIE/src/run.py", line 353, in <module>
    train_collie(
  File "/home/chrisjihee/proj/GoLLIE/src/run.py", line 41, in train_collie
    model, tokenizer = load_model(
                       ^^^^^^^^^^^
  File "/home/chrisjihee/proj/GoLLIE/src/model/load_model.py", line 370, in load_model
    from src.model.patch_models.modeling_flash_llama import LlamaForCausalLM as LlamaForCausalLMFlash
  File "/home/chrisjihee/proj/GoLLIE/src/model/patch_models/modeling_flash_llama.py", line 61, in <module>
    raise ImportError("Please install Flash Attention: `pip install flash-attn --no-build-isolation`")
ImportError: Please install Flash Attention: `pip install flash-attn --no-build-isolation`

Process finished with exit code 1
