(GoLLIE) chrisjihee@ptlm2:~/proj/GoLLIE$ bash bash_scripts/GoLLIE-7B_CodeLLaMA.sh
CUDA_VISIBLE_DEVICES
INFO:root:Sys args ['/home/chrisjihee/proj/GoLLIE/src/run.py', 'configs/model_configs/GoLLIE-7B_CodeLLaMA.yaml']
INFO:root:Loading yaml config configs/model_configs/GoLLIE-7B_CodeLLaMA.yaml
/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
INFO:root:Loading codellama/CodeLlama-7b-hf model...
INFO:root:Loading model model from codellama/CodeLlama-7b-hf
INFO:root:We will load the model using the following device map: None and max_memory: None
WARNING:root:Tokenizer does not have a pad token, we will use the unk token as pad token.
INFO:root:Bits and Bytes config: {
    "quant_method": "bitsandbytes",
    "_load_in_8bit": false,
    "_load_in_4bit": true,
    "llm_int8_threshold": 6.0,
    "llm_int8_skip_modules": null,
    "llm_int8_enable_fp32_cpu_offload": false,
    "llm_int8_has_fp16_weight": false,
    "bnb_4bit_quant_type": "nf4",
    "bnb_4bit_use_double_quant": true,
    "bnb_4bit_compute_dtype": "bfloat16",
    "bnb_4bit_quant_storage": "uint8",
    "load_in_4bit": true,
    "load_in_8bit": false
}
WARNING:root:Model codellama/CodeLlama-7b-hf is an decoder-only model. We will load it as a CausalLM model.
>>>> Flash Attention installed
>>>> Flash RoPE installed
WARNING:root:Using Flash Attention for LLaMA model.
`low_cpu_mem_usage` was None, now default to True since model is quantized.
LlamaForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.10s/it]
INFO:root:Model dtype: torch.bfloat16
INFO:root:Total model memory footprint: 3763.093698 MB
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
INFO:root:No pretrained LORA weights provided, we will initialize the weights randomly.
WARNING:root:You provided 'all' as target modules, we will use all the model to which LoRA can be applied.
INFO:root:
LoRA config:
{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='codellama/CodeLlama-7b-hf', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=8, target_modules={'q_proj', 'gate_proj', 'k_proj', 'down_proj', 'v_proj', 'up_proj', 'o_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}

INFO:root:---> Trainable params: 19988480 || all params: 3520532480 || trainable%: 0.567769

INFO:root:Loading datasets...
INFO:root:We will train CoLLIE on 5 datasets: data/processed_w_examples/bc5cdr.ner.train.jsonl, data/processed_w_examples/conll03.ner.train.jsonl, data/processed_w_examples/ncbidisease.ner.train.jsonl, data/processed_w_examples/ontonotes5.ner.train.jsonl, data/processed_w_examples/wnut17.ner.train.jsonl
INFO:root:We will validate CoLLIE on 1 datasets: data/processed_w_examples/conll03.ner.dev.jsonl
INFO:root:Training dataset will be loaded with. 'ignore_pad_token_for_loss': True and 'prompt_loss_weight': 0.0
INFO:root:Found 3 pre-computed epoch datasets.
  Tokenizing bc5cdr.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
  Tokenizing bc5cdr.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
  Tokenizing bc5cdr.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
INFO:root:Loaded [4561, 4561, 4561] examples from data/processed_w_examples/bc5cdr.ner.train.jsonl
INFO:root:Found 3 pre-computed epoch datasets.
  Tokenizing conll03.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:02
  Tokenizing conll03.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:02
  Tokenizing conll03.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:02
INFO:root:Loaded [14041, 14041, 14041] examples from data/processed_w_examples/conll03.ner.train.jsonl
INFO:root:Found 3 pre-computed epoch datasets.
  Tokenizing ncbidisease.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
  Tokenizing ncbidisease.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
  Tokenizing ncbidisease.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
INFO:root:Loaded [5433, 5433, 5433] examples from data/processed_w_examples/ncbidisease.ner.train.jsonl
INFO:root:Found 3 pre-computed epoch datasets.
  Tokenizing ontonotes5.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:06
  Tokenizing ontonotes5.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:06
  Tokenizing ontonotes5.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:06
INFO:root:Loaded [30000, 30000, 30000] examples from data/processed_w_examples/ontonotes5.ner.train.jsonl
INFO:root:Found 3 pre-computed epoch datasets.
  Tokenizing wnut17.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
  Tokenizing wnut17.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
  Tokenizing wnut17.ner.train ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
INFO:root:Loaded [3394, 3394, 3394] examples from data/processed_w_examples/wnut17.ner.train.jsonl
  Tokenizing conll03.ner.dev ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 0:00:00 0:00:00
INFO:root:Loaded [3250] examples from data/processed_w_examples/conll03.ner.dev.jsonl
/home/chrisjihee/proj/GoLLIE/src/trainer.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CollieTrainer.__init__`. Use `processing_class` instead.
  super().__init__(
Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Tracking run with wandb version 0.18.7
wandb: W&B syncing is set to `offline` in this directory.
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|                                                                                                                                                                                                                                               | 0/1347 [00:00<?, ?it/s]Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/chrisjihee/proj/GoLLIE/src/run.py", line 353, in <module>
    train_collie(
  File "/home/chrisjihee/proj/GoLLIE/src/run.py", line 127, in train_collie
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/transformers/trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/chrisjihee/miniforge3/envs/GoLLIE/lib/python3.11/site-packages/transformers/trainer.py", line 3579, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: CollieTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
wandb:
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/chrisjihee/proj/GoLLIE/wandb/offline-run-20241126_204116-1t9imlo4
wandb: Find logs at: wandb/offline-run-20241126_204116-1t9imlo4/logs
(GoLLIE) chrisjihee@ptlm2:~/proj/GoLLIE$

